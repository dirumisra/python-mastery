{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa17a637",
   "metadata": {},
   "source": [
    "**Paths: pathlib (preferred) vs os.path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ff3a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True False\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Construct a safe, cross-platform file path: data/in/file.txt\n",
    "# Using '/' with Path objects automatically joins directories in a way that works on any OS (Windows, Linux, Mac)\n",
    "p = Path(\"data\") / \"in\" / \"file.txt\"\n",
    "\n",
    "# Print whether the path exists, is a file, and is a directory\n",
    "# Returns: (False, False, False) initially, before file is created\n",
    "print(p.exists(), p.is_file(), p.is_dir())\n",
    "\n",
    "# Create the parent directories of the file path, if they don't exist\n",
    "# parents=True → creates all intermediate directories (like mkdir -p in shell)\n",
    "# exist_ok=True → doesn't raise an error if the directory already exists\n",
    "p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write text to the file, creating the file if it doesn't exist (or overwriting it)\n",
    "# encoding='utf-8' ensures consistent behavior across platforms\n",
    "p.write_text(\"hello\", encoding=\"utf-8\")\n",
    "\n",
    "# Read and print the content from the file\n",
    "print(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# --- File searching using glob/rglob ---\n",
    "\n",
    "# Recursively search for all .csv files under the \"data\" directory\n",
    "# rglob(\"*.csv\") looks in all subdirectories; use glob(\"*.csv\") for non-recursive\n",
    "for f in Path(\"data\").rglob(\"*.csv\"):\n",
    "    # Print the file name and its size in bytes\n",
    "    print(f.name, f.stat().st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecd6a7",
   "metadata": {},
   "source": [
    "**OS / filesystem helpers: os, shutil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a87d9d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Created. True\n",
      "Renamed file exists: True\n",
      "File still exists after delete: False\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Create a sample file using pathlib (safe and cross-platform)\n",
    "original_file = Path(\"sourcefile/old.txt\")\n",
    "original_file.write_text(\"This is a test file.\", encoding=\"utf-8\")\n",
    "print(f\"File Created. {original_file.exists()}\")  # Should print: True\n",
    "\n",
    "try:\n",
    "    # Step 2: Rename the file using os.rename()\n",
    "    # This will rename 'old.txt' to 'new.txt' inside the 'sourcefile' directory\n",
    "    os.rename(\"sourcefile/old.txt\", \"sourcefile/new.txt\")\n",
    "    print(f\"Renamed file exists: {Path('sourcefile/new.txt').exists()}\")  # Check if new.txt exists after renaming\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file to rename was not found.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied while renaming the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred while renaming: {e}\")\n",
    "\n",
    "try:\n",
    "    # Step 3: Remove the renamed file\n",
    "    # This deletes 'new.txt' from the filesystem\n",
    "    os.remove(\"sourcefile/new.txt\")\n",
    "    print(f\"File still exists after delete: {Path('sourcefile/new.txt').exists()}\")  # Should print: False\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file to delete was not found.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied while deleting the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred while deleting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed28b5f",
   "metadata": {},
   "source": [
    "**CSV files: csv module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edc2c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dhiraj\n",
      "2 pooja\n"
     ]
    }
   ],
   "source": [
    "import csv  # Import the built-in CSV module to handle CSV file operations\n",
    "\n",
    "# --- Define Data to Write --- \n",
    "# 'rows' is a list of dictionaries, where each dictionary represents a row in the CSV file\n",
    "rows = [\n",
    "    {\"id\": 1, \"name\": \"dhiraj\"},  # First row with id and name\n",
    "    {\"id\": 2, \"name\": \"pooja\"},   # Second row with id and name\n",
    "]\n",
    "\n",
    "# --- Write CSV with header ---\n",
    "# Open the 'people.csv' file in write mode ('w') with UTF-8 encoding\n",
    "# `newline=\"\"` ensures no extra blank lines are added between rows on Windows\n",
    "with open(\"sourcefile/people.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # Create a DictWriter object to write dictionaries into the CSV file\n",
    "    # 'fieldnames' defines the order and names of the columns (header row)\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"id\", \"name\"])\n",
    "\n",
    "    # Write the header row (column names) to the CSV file\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the rows of data from the 'rows' list into the CSV file\n",
    "    writer.writerows(rows)\n",
    "\n",
    "# --- Read CSV ---\n",
    "# Open the 'people.csv' file in read mode ('r') to read the data back\n",
    "# Again, we use `newline=\"\"` and `encoding=\"utf-8\"` for consistent line endings and encoding\n",
    "with open(\"sourcefile/people.csv\", \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # Create a DictReader object to read the CSV file into dictionaries (where the keys are column names)\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    # Loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Access the 'id' and 'name' fields for each row and print them\n",
    "        print(row[\"id\"], row[\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1ab61",
   "metadata": {},
   "source": [
    "**JSON files: json module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06303395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # Import the built-in json module to work with JSON data in Python\n",
    "\n",
    "# --- Create a Python dictionary representing structured data ---\n",
    "data = {\n",
    "    \"id\": 1,                         # An integer field\n",
    "    \"name\": \"dhiraj\",               # A string field\n",
    "    \"skills\": [\"python\", \"AI\"]      # A list field (array in JSON)\n",
    "}\n",
    "\n",
    "# --- Write JSON to a file ---\n",
    "# Open (or create) the file 'user.json' inside the 'sourcefile' folder for writing ('w')\n",
    "# UTF-8 encoding ensures proper support for Unicode characters\n",
    "with open(\"sourcefile/user.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # Serialize the 'data' dictionary into JSON and write it to the file\n",
    "    # - ensure_ascii=False allows non-ASCII characters to be written as-is (not escaped)\n",
    "    # - indent=2 formats the JSON with indentation (pretty-printed)\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- Read JSON from the file ---\n",
    "# Open the same file again, this time for reading ('r')\n",
    "with open(\"sourcefile/user.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # Deserialize the JSON content back into a Python dictionary\n",
    "    obj = json.load(f)\n",
    "\n",
    "# At this point, 'obj' is a Python dict equivalent to the original 'data'\n",
    "# You can now access it using obj[\"id\"], obj[\"skills\"], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0ed7d",
   "metadata": {},
   "source": [
    "**Binary formats & compression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36ea8854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed text\n"
     ]
    }
   ],
   "source": [
    "import gzip  # For working with .gz (GZIP) compressed files\n",
    "import bz2   # For working with .bz2 (Bzip2) compressed files\n",
    "\n",
    "# --- Write text to a gzip-compressed file ---\n",
    "# Open 'data.txt.gz' inside 'sourcefile/' in write-text mode ('wt')\n",
    "# 'wt' = write text, as opposed to 'wb' (write binary)\n",
    "# UTF-8 encoding ensures proper handling of text characters\n",
    "with gzip.open(\"sourcefile/data.txt.gz\", \"wt\", encoding=\"utf-8\") as f:\n",
    "    # Write a string to the compressed file\n",
    "    f.write(\"Compressed text\")\n",
    "\n",
    "# --- Read text back from the gzip-compressed file ---\n",
    "# Open the same file in read-text mode ('rt')\n",
    "# 'rt' = read text; again, UTF-8 decoding is used\n",
    "with gzip.open(\"sourcefile/data.txt.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    # Read and print the content from the compressed file\n",
    "    print(f.read())  # Output: Compressed text\n",
    "\n",
    "# --- Write text to a bz2-compressed file ---\n",
    "# Open 'data.txt.bz2' in write-text mode with UTF-8 encoding\n",
    "# Bzip2 provides better compression ratio than gzip (usually slower)\n",
    "with bz2.open(\"sourcefile/data.txt.bz2\", \"wt\", encoding=\"utf-8\") as f:\n",
    "    # Write a string to the .bz2 compressed file\n",
    "    f.write(\"hello bz2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c9289",
   "metadata": {},
   "source": [
    "**Zip files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5acfb712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sourcefile/people.csv', 'sourcefile/user.json']\n"
     ]
    }
   ],
   "source": [
    "import zipfile  # Module for handling ZIP archive files\n",
    "from pathlib import Path  # For working with filesystem paths in a clean, cross-platform way\n",
    "\n",
    "# --- Step 1: Create a ZIP file and add files to it ---\n",
    "# 'w' mode means write — it will create a new zip file (overwrites if it already exists)\n",
    "# compression=ZIP_DEFLATED enables standard ZIP compression\n",
    "with zipfile.ZipFile(\"sourcefile/bundle.zip\", \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    # Add files to the archive\n",
    "    # These files must already exist in the specified location\n",
    "    z.write(\"sourcefile/people.csv\")  # Add CSV file to the archive\n",
    "    z.write(\"sourcefile/user.json\")   # Add JSON file to the archive\n",
    "\n",
    "# --- Step 2: Extract all files from the ZIP archive ---\n",
    "# This will extract all files into the 'unzipped' folder (creates it if it doesn't exist)\n",
    "with zipfile.ZipFile(\"sourcefile/bundle.zip\") as z:\n",
    "    z.extractall(\"unzipped\")  # Extracts all contents to the \"unzipped/\" folder\n",
    "\n",
    "# --- Step 3: List the contents of the ZIP archive ---\n",
    "# Opens the zip again in read mode (default)\n",
    "with zipfile.ZipFile(\"sourcefile/bundle.zip\") as z:\n",
    "    # namelist() returns a list of all file names in the archive\n",
    "    print(z.namelist())  # Output: ['sourcefile/people.csv', 'sourcefile/user.json']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24001d",
   "metadata": {},
   "source": [
    "**Tar files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18adaef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tarfile\\nwith tarfile.open(\"logs.tar.gz\", \"w:gz\") as t:\\n    t.add(\"logs\", arcname=\"logs\")'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tarfile\n",
    "with tarfile.open(\"logs.tar.gz\", \"w:gz\") as t:\n",
    "    t.add(\"logs\", arcname=\"logs\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c2e8ef",
   "metadata": {},
   "source": [
    "Pickle (Python object serialization) — caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c91e8aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [1, 2], 'b': {'x': 3}}\n"
     ]
    }
   ],
   "source": [
    "import pickle  # Import the 'pickle' module to handle object serialization and deserialization\n",
    "\n",
    "# Define a sample data structure (a dictionary)\n",
    "data = {\"a\": [1, 2], \"b\": {\"x\": 3}}\n",
    "\n",
    "# Open a file named 'obj.pkl' in write-binary mode ('wb') to store the serialized object\n",
    "with open(\"sourcefile/obj.pkl\", \"wb\") as f:  \n",
    "    # Use pickle.dump() to serialize (convert into a byte stream) the 'data' object\n",
    "    # and write it to the file\n",
    "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # `protocol=pickle.HIGHEST_PROTOCOL` ensures the latest, most efficient pickle format is used.\n",
    "\n",
    "# Now let's read the object back from the file.\n",
    "# Open the file 'obj.pkl' in read-binary mode ('rb') to load the serialized object\n",
    "with open(\"sourcefile/obj.pkl\", \"rb\") as f:\n",
    "    # Use pickle.load() to deserialize (convert the byte stream back) the object from the file\n",
    "    loaded = pickle.load(f)\n",
    "\n",
    "# After this point, 'loaded' should hold the same value as 'data'\n",
    "print(loaded)  # Output the loaded data to confirm it's the same as the original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33343b9",
   "metadata": {},
   "source": [
    "**Temporary files & dirs: tempfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4fb65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp content\n",
      "Temp path: C:\\Users\\dhira\\AppData\\Local\\Temp\\tmpn5l7_s7n\n",
      "Dir contents: [WindowsPath('C:/Users/dhira/AppData/Local/Temp/tmpwuunt5zx/a.txt')]\n"
     ]
    }
   ],
   "source": [
    "import tempfile  # Import the tempfile module to create temporary files and directories\n",
    "import pathlib   # Import the pathlib module for easy path manipulation\n",
    "\n",
    "# Create a temporary file using NamedTemporaryFile\n",
    "with tempfile.NamedTemporaryFile(\"w+\", delete=True, encoding=\"utf-8\") as tf:\n",
    "    # 'w+' mode: Open the file for both reading and writing in text mode\n",
    "    # 'delete=True': The file will be deleted as soon as it's closed\n",
    "    # 'encoding=\"utf-8\"': The file is opened with UTF-8 encoding for reading/writing text\n",
    "    \n",
    "    # Write some content to the temporary file\n",
    "    tf.write(\"temp content\")\n",
    "    \n",
    "    # Move the file pointer back to the beginning of the file to read it\n",
    "    tf.seek(0)\n",
    "    \n",
    "    # Read the content of the temporary file and print it\n",
    "    print(tf.read())  # Output: 'temp content'\n",
    "    \n",
    "    # Print the temporary file's name (full path)\n",
    "    print(\"Temp path:\", tf.name)  # The file name (path) is printed (this is a temp file)\n",
    "\n",
    "# Create a temporary directory using TemporaryDirectory\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    # 'TemporaryDirectory()' creates a temporary directory that will be cleaned up after use\n",
    "    \n",
    "    # Create a new path object (using pathlib) for a file 'a.txt' in the temporary directory\n",
    "    p = pathlib.Path(td) / \"a.txt\"  # Combining path and filename\n",
    "    \n",
    "    # Write some text to 'a.txt' using pathlib's write_text method\n",
    "    p.write_text(\"hi\", encoding=\"utf-8\")  # Write 'hi' to the file in UTF-8 encoding\n",
    "    \n",
    "    # List and print all files in the temporary directory\n",
    "    print(\"Dir contents:\", list(p.parent.iterdir()))  # Iterates over the contents of the directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f2945",
   "metadata": {},
   "source": [
    "**Memory-mapped files: mmap (advanced, huge files)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b67a6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xa3\\x1c\\x06\\xbdF>9#\\xbc\\x1a'\n"
     ]
    }
   ],
   "source": [
    "import mmap  # Import the 'mmap' module for memory-mapped file access\n",
    "\n",
    "# Open the file 'sourcefile/big.bin' in read and write binary mode ('r+b')\n",
    "with open(\"sourcefile/big.bin\", \"r+b\") as f:\n",
    "    # Create a memory-mapped object for the file. \n",
    "    # f.fileno() returns the file descriptor for the open file\n",
    "    # 0 means map the entire file into memory\n",
    "    mm = mmap.mmap(f.fileno(), 0)  \n",
    "\n",
    "    # Print the first 10 bytes of the memory-mapped file.\n",
    "    # 'mm[:10]' gives a slice of the first 10 bytes as a byte object\n",
    "    print(mm[:10])  \n",
    "\n",
    "    # mm[0:4] reads bytes from position 0 to 4 (but doesn't print them)\n",
    "    # This is just accessing the data but not doing anything with it\n",
    "    mm[0:4]  # Access bytes 0-3 of the mapped file (no operation on the result)\n",
    "\n",
    "    # Close the memory-mapped file to free resources\n",
    "    mm.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fcb57",
   "metadata": {},
   "source": [
    "**File locking (advanced, OS-dependent)**\n",
    "- Windows: msvcrt.locking\n",
    "- Unix: fcntl.flock Or use a cross-platform library (e.g., portalocker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c61b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import portalocker  # Import the portalocker module to handle file locking\n",
    "import time  # Import time module for simulating delays\n",
    "\n",
    "# Open the file 'shared.log' in append mode ('a'), which allows adding new lines without overwriting\n",
    "with open(\"sourcefile/shared.log\", \"a\", encoding=\"utf-8\") as f:\n",
    "    # Lock the file with an exclusive lock (LOCK_EX)\n",
    "    portalocker.lock(f, portalocker.LOCK_EX)  # Exclusive lock: only one process can hold this lock at a time\n",
    "    \n",
    "    try:\n",
    "        # Write a line to the file\n",
    "        f.write(\"line\\n\")\n",
    "        \n",
    "        # Simulate a long write operation with a 1-second sleep\n",
    "        time.sleep(1)  # Sleep is used to simulate a time-consuming operation (e.g., network I/O)\n",
    "    \n",
    "    finally:\n",
    "        # Unlock the file once the writing is done\n",
    "        portalocker.unlock(f)  # Release the lock so other processes/threads can access the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f9461",
   "metadata": {},
   "source": [
    "**Logging file operations (robust apps)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9878a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=\"app.log\",  # Log messages will be saved to 'app.log' in the current directory.\n",
    "    level=logging.INFO,  # Only log messages of INFO level or higher (INFO, WARNING, ERROR, CRITICAL).\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"  # Log format: timestamp, log level (INFO, ERROR), and the actual message.\n",
    ")\n",
    "\n",
    "# Function to save the report to a file\n",
    "def save_report(text: str, path: Path):\n",
    "    try:\n",
    "        # Create the parent directories if they don't exist.\n",
    "        # 'parents=True' ensures all missing directories are created.\n",
    "        # 'exist_ok=True' prevents an error if the directories already exist.\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write the text content to the specified file path.\n",
    "        # 'path.write_text()' writes the string 'text' into the file at 'path'.\n",
    "        # It also ensures that the file is saved with UTF-8 encoding.\n",
    "        path.write_text(text, encoding=\"utf-8\")\n",
    "        \n",
    "        # Log an informational message after successfully saving the report.\n",
    "        # This is helpful for keeping track of the file-saving process.\n",
    "        logging.info(\"Saved report to %s\", path)\n",
    "    except Exception as e:\n",
    "        # If an error occurs (e.g., permission issues, invalid path), the exception is caught.\n",
    "        # The error message along with the traceback is logged.\n",
    "        # 'logging.exception' automatically includes the stack trace of the exception.\n",
    "        logging.exception(\"Failed to save report: %s\", e)\n",
    "\n",
    "# Call the function to save the report\n",
    "# The text \"Hello\" is written to the file \"reports/2025-10-17.txt\".\n",
    "# If the directories (e.g., 'reports') don't exist, they will be created.\n",
    "save_report(\"Hello\", Path(\"reports/2025-10-17.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fd094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
